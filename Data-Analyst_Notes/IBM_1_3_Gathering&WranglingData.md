
# GATHERING DATA #

### SUMMARY ###

- The process of identifying data begins by determining the information that needs to be collected, which in turn is determined by the goal you seek to achieve. 

- Having identified the data, your next step is to identify the sources from which you will extract the required data and define a plan for data collection. Decisions regarding the timeframe over which you need your data set, and how much data would suffice for arriving at a credible analysis also weigh in at this stage.  

- Data Sources can be internal or external to the organization, and they can be primary, secondary, or third-party, depending on whether you are obtaining the data directly from the original source, retrieving it from externally available data sources, or purchasing it from data aggregators. 

- Some of the data sources from which you could be gathering data include databases, the web, social media, interactive platforms, sensor devices, data exchanges, surveys and observation studies. 

- Data that has been identified and gathered from the various data sources is combined using a variety of tools and methods to provide a single interface using which data can be queried and manipulated. 

- The data you identify, the source of that data, and the practices you employ for gathering the data have implications for quality, security, and privacy, which need to be considered at this stage. 



## IDENTIFYING DATA FOR ANALYSIS ##

> At this stage, you have an understanding of the problem and the desired outcome—you know “Where you are” and “Where you want to be.“ You also have a well-defined metric—you know “What will be measured,” and “How it will be measured.” The next step is for you is to identify the data you need for your use case. 

- The process of identifying data begins by determining the information you want to collect. In this step, you make decisions regarding (a) the specific information you need; and (b) the possible sources for this data. Your goals determine the answers to these questions. 
    · USE CASE: Let’s take the example of a product company that wants to create targeted marketing campaigns based on the age group that buys their products the most. Their goal is to design reach-outs that appeal most to this segment and encourages them to further influence their friends and peers into buying these products. Based on this use case, some of the obvious information that you will identify includes the customer profile, purchase history, location, age, education, profession, income, and marital status, for example. To ensure you gain even greater insights into this segment, you may also decide to collect the customer complaint data for this segment to understand the kind of issues they face because this could discourage them from recommending your products. To know how satisfied they were with the resolution of their issues, you could collect the ratings from the customer service surveys. Taking this a step forward, you may want to understand how these customers talk about your products on social media and how many of their connections engage with them in these discussions, for example, the likes, shares, and comments their posts receive. 
    
- The next step in the process is to define a plan for COLLECTING data. You need to establish a timeframe for collecting the data you have identified. Some of the data you need may be required on an ongoing basis and some over a defined period of time:
    ·For collecting website visitor data, for example, you may need to have the numbers refreshed in real-time. 
    · But if you’re tracking data for a specific event, you have a definite beginning and end date for collecting the data. 
    
- In this step, you can also define HOW MUCH data would be sufficient for you to reach a credible analysis. 
    · Is the volume defined by the segment, for example, all customers within the age range of 21 to 30 years; or a dataset of a hundred thousand customers within the age range of 21 to 30. 
    · You can also use this step to define the dependencies, risks, mitigation plan, and several other such factors that are relevant to your initiative. The purpose of the plan should be to establish the clarity you need for execution. 
    
- The third step in the process is for you to determine your data collection METHODS. 
    · You will define how you will collect the data from the data sources you have identified, such as internal systems, social media sites, or third-party data providers. 
    · Your methods will depend on the type of data, the timeframe over which you need the data, and the volume of data. 
    
- Once your plan and data collection methods are finalized, you can implement your data collection strategy and start collecting data. You will be making updates to your plan as you go along because conditions evolve as you implement the plan on the ground. The data you identify, the source of that data, and the practices you employ for gathering the data have implications for quality, security, and privacy. None of these are one-time considerations but are relevant through the life cycle of the data analysis process. 

- Working with data from disparate sources without considering how it measures against the QUALITY metric can lead to failure. In order to be reliable, data needs to be free of errors, accurate, complete, relevant, and accessible. You need to define the quality traits, the metric, and the checkpoints in order to ensure that your analysis is going to be based on quality data. 

- You also need to watch out for issues pertaining to data GOVERNANCE, such as, security, regulation, and compliances. Data Governance policies and procedures relate to the usability, integrity, and availability of data. Penalties for non-compliance can run into millions of dollars and can hurt the credibility of not just your findings, but also your organization. 

- Another important consideration is data PRIVACY. Data you collect needs to check the boxes for confidentiality, license for use, and compliance to mandated regulations. Checks, validations, and an auditable trail needs to be planned. Loss of trust in the data used for analysis can compromise the process, result in suspect findings, and invite penalties. 


> Identifying the right data is a very important step of the data analysis process. Done right, it will ensure that you are able to look at a problem from multiple perspectives and your findings are credible and reliable.



## DATA SOURCES ##

Data sources can be internal or external to the organization, and they can be primary, secondary or third party sources of data. 
Let's look at a couple of examples: 

- PRIMARY data refers to information obtained directly by you from the source. 
    · This could be from internal sources such as data from the organization, CRM, HR or workflow applications. 
    · It could also include data you gather directly through surveys, interviews, discussions, observations and focus groups. 
    
- SECONDARY data refers to information retrieved from existing sources.
    · Such as external databases, research articles, publications, training material and Internet searches, or financial records available as public data. 
    · This could also include data collected through externally conducted surveys, interviews, discussions, observations and focus groups. 

- THIRD party data is data you purchased from aggregators who collect data from various sources and combine it into comprehensive datasets purely for the purpose of selling the data. 


- Now will look at some of the different sources from which you could be gathering data: 

    · Most organizations have INTERNAL APPLICATIONS for managing their processes, workflows and customers. 

    · EXTERNAL DATABASES are available on a subscription basis or for purchase. 

    · A significant number of businesses have or are currently moving to THE CLOUD, which is increasingly becoming a source for accessing real time information and on demand insights. 

    · THE WEB is a source of publicly available data that is available to companies and individuals for free or commercial use. The Web is a rich source of data available in the public domain. These could include textbooks, government records, papers, and articles that are for public consumption.

    · SOCIAL MEDIA SITES and interactive platforms such as Facebook, Twitter, Google, YouTube and Instagram, are increasingly being used to source user data and opinions. Businesses are using these data sources for quantitative and qualitative insights, and existing and potential customers. 

    · SENSOR DATA produced by wearable devices, smart buildings, smart cities, smart phones, medical devices, even household appliances is a widely used source of data. 

    · DATA EXCHANGE is a source of 3rd party data that involves the voluntary sharing of data between data providers and data consumers, individuals, organizations and governments could be both data providers and data consumers. The data that is exchanged could include data coming from business applications, sensor devices, social media activity, location data, or consumer behavior data. 

    · SURVEYS gather information through questionnaires distributed to a select group of people. For example, gauging the interest of existing customers in spending on an updated version of a product. Surveys can be web or paper based. 

    · CENSUS data is also a commonly used source for gathering household data, such as wealth and income or population data, for example. 
    
    · INTERVIEWS are source for gathering qualitative data, such as the participants opinions and experiences. For example, an interview conducted to understand the day-to-day challenges faced by a customer service executive. Interviews could be telephonic over the Web or face to face observation. 
    
    · STUDIES include monitoring participants in a specific environment or while performing a particular task. For example, observing users navigate an E Commerce site to assess the. Ease with which they are able to find products and make a purchase data from surveys, interviews, an observation. Studies could be available as primary, secondary and 3rd party data. 
    
    > Data sources have never been as dynamic and diverse as they are today. They are also evolving continuously. Supplementing your primary data with secondary and 3rd party data sources can help you explore problems and solutions in new and meaningful ways.



## HOW TO GATHER AND IMPORT ##

> In this video, we will learn about the different methods and tools available for gathering data from the data sources discussed earlier in the course—such as databases, the web, sensor data, data exchanges, and several other sources leveraged for specific data needs. We will also learn about importing data into different types of data repositories. 

- SQL, or Structured Query Language: is a querying language used for extracting information from relational databases. 
    · SQL offers simple commands to specify what is to be retrieved from the database, the table from which it needs to be extracted, grouping records with matching values, dictating the sequence in which the query results are displayed, and limiting the number of results that can be returned by the query, amongst a host of other features and functionalities. 
    
- Non-relational databases can be queried using SQL or SQL-like query tools. 
    · Some non-relational databases come with their own querying tools such as CQL for Cassandra and GraphQL for Neo4J. 
    
- Application Programming Interfaces (or APIs) are also popularly used for extracting data from a variety of data sources. 
    · APIs are invoked from applications that require the data and access an end-point containing the data. End-points can include databases, web services, and data marketplaces. APIs are also used for data validation. 
    · For example, a data analyst may utilize an API to validate postal addresses and zip codes. 

- Web scraping, also known as screen scraping or web harvesting, is used for downloading specific data from web pages based on defined parameters. Among other things, web scraping is used to extract data such as text, contact information, images, videos, podcasts, and product items from a web property. 

- RSS feeds are another source typically used for capturing updated data from online forums and news sites where data is refreshed on an ongoing basis. 

- Data streams are a popular source for aggregating constant streams of data flowing from sources such as instruments, IoT devices and applications, and GPS data from cars. Data streams and feeds are also used for extracting data from social media sites and interactive platforms. 

- Data Exchange platforms allow the exchange of data between data providers and data consumers. 
    · Data Exchanges have a set of well-defined exchange standards, protocols, and formats relevant for exchanging data. 
    · These platforms not only facilitate the exchange of data, they also ensure that security and governance are maintained. 
    · They provide data licensing workflows, de-identification and protection of personal information, legal frameworks, and a quarantined analytics environment. 
    · Examples of popular data exchange platforms include AWS Data Exchange, Crunchbase, Lotame, and Snowflake. 
    
- Numerous other data sources can be tapped into for specific data needs. 
    · For marketing trends and ad spending, for example, research firms like Forrester and Business Insider are known to provide reliable data. 
    · Research and advisory firms such as Gartner and Forrester are widely trusted sources for strategic and operational guidance. 
    · Similarly, there are many trusted names in the areas of user behavior data, mobile and web usage, market surveys, and demographic studies. 
    

> Data that has been identified and gathered from the various data sources now needs to be loaded or imported into a data repository before it can be wrangled, mined, and analyzed. 

- The IMPORTING process involves combining data from different sources to provide a combined view and a single interface using which you can query and manipulate the data. Depending on the data type, the volume of data, and the type of destination repository, you may need varying tools and methods. Specific data repositories are optimized for certain types of data: 

    · RELATIONAL databases store structured data with a well-defined schema. If you’re using a relational database as the destination system, you will only be able to store structured data, such as data from OLTP systems, spreadsheets, online forms, sensors, network and web logs. Structured data can also be stored in NoSQL. 
    
    · SEMI-STRUCTURED data is data that has some organizational properties but not a rigid schema, such as, data from emails, XML, zipped files, binary executables, and TCP/IP protocols. Semi-structured can be stored in NoSQL clusters. XML and JSON are commonly used for storing and exchanging semi-structured data. JSON is also the preferred data type for web services. 
    
    · UNSTRUCTURED data is data that does not have a structure and cannot be organized into a schema, such as data from web pages, social media feeds, images, videos, documents, media logs, and surveys. NoSQL databases and Data Lakes provide a good option to store and manipulate large volumes of unstructured data. Data lakes can accommodate all data types and schema. ETL tools and data pipelines provide automated functions that facilitate the process of importing data. 
    
- TOOLS such as Talend and Informatica, and programming languages such as Python and R, and their libraries, are widely used for importing data.




# WRANGLING DATA #

### SUMMARY ###

Once the data you identified is gathered and imported, your next step is to make it analysis-ready. This is where the process of Data Wrangling, or Data MUNGING, comes in. Data Wrangling is an iterative process that involves data exploration, transformation, and validation.  

Transformation of raw data includes the tasks you undertake to:
- Structurally manipulate and combine the data using Joins and Unions.
- Normalize data, that is, clean the database of unused and redundant data.
- Denormalize data, that is, combine data from multiple tables into a single table so that it can be queried faster. 
- Clean data, which involves profiling data to uncover quality issues, visualizing data to spot outliers, and fixing issues such as missing values, duplicate data, irrelevant data, inconsistent formats, syntax errors, and outliers.
- Enrich data, which involves considering additional data points that could add value to the existing data set and lead to a more meaningful analysis.

A variety of software and tools are available for the Data Wrangling process. Some of the popularly used ones include Excel Power Query, Spreadsheets, OpenRefine, Google DataPrep, Watson Studio Refinery, Trifacta Wrangler, Python, and R, each with their own set of characteristics, strengths, limitations, and applications. 



## WHAT IS DATA WRANGLING? ##

- Data wrangling, also known as data munging, is an iterative process that involves data exploration, transformation, validation, and making it available for a credible and meaningful analysis. 
    · It includes a range of tasks involved in preparing raw data for a clearly defined purpose, where raw data at this stage is data that has been collated through various data sources in a data repository. 
    · Data wrangling captures a range of tasks involved in preparing data for analysis. Typically, it is a 4-step process that involves: Discovery, Transformation, Validation, and Publishing. 
    
- The DISCOVERY phase, also known as the Exploration phase, is about understanding your data better with respect to your use case. The objective is to figure out specifically how best you can clean, structure, organize, and map the data you have for your use case. 

- The TRANSFORMATION phase, forms the bulk of the data wrangling process. It involves the tasks you undertake to transform the data, such as structuring, normalizing, denormalizing, cleaning, and enriching the data. Let’s begin with the first transformation task:

    · STRUCTURING: This task includes actions that change the form and schema of your data. The incoming data can be in varied formats. You might, for example, have some data coming from a relational database and some data from Web APIs. In order to merge them, you will need to change the form or schema of your data. This change may be as simple as changing the order of fields within a record or dataset or as complex as combining fields into complex structures. Joins and Unions are the most common structural transformations used to combine data from one or more tables. How they combine the data is different. 
        * JOINS combine columns. When two tables are joined together, columns from the first source table are combined with columns from the second source table—in the same row. So, each row in the resultant table contains columns from both tables. 
        * UNIONS combine rows. Rows of data from the first source table are combined with rows of data from the second source table into a single table. Each row in the resultant table is from one source table or another. 
        
    · NORMALIZATION: Focuses on cleaning the database of unused data and reducing redundancy and inconsistency. Data coming from transactional systems, for example, where a number of insert, update, and delete operations are performed on an ongoing basis, are highly normalized. 
    
    · DENORMALIZATION: Is used to combine data from multiple tables into a single table so that it can be queried faster. For example, normalized data coming from transactional systems is typically denormalized before running queries for reporting and analysis.
    
    · CLEANING: Cleaning tasks are actions that fix irregularities in data in order to produce a credible and accurate analysis. Data that is inaccurate, missing, or incomplete can skew the results of your analysis and need to be considered. It could also be that the data is biased, or has null values in relevant fields, or have outliers. 
        * For EXAMPLE, you may want to find out the demographic information on the sale of a certain product, but the data you have received does not capture the gender. You either need to source this data point and merge it with your existing dataset, or you may need to remove, and not consider the records with this field missing. 
        
    · ENRICHING: When you consider the data you have, to look at additional data points that could make your analysis more meaningful, you are looking at enriching your data. 
        * For EXAMPLE, in a large organization with information fragmented across systems, you may need to enrich the dataset provided by one system with information available in other systems, or even public datasets. Consider a scenario where you sell IT peripherals to businesses and want to analyze the buying patterns of your customers over the last five years. You have the customer master and transaction tables from where you’ve captured the customer information and purchase history. 

    Supplementing your dataset with the performance data of these businesses, possibly available as a public dataset, could be valuable for you to understand factors influencing their purchase decisions. Inserting METADATA also enriches data. 
        * For EXAMPLE, computing a sentiment score from a customer feedback log, collecting geo-based weather data from a resorts location to analyze occupancy trends, or capturing published time and tags for a blog post. 
        
        
- The next phase is VALIDATION. This is where you check the quality of the data post structuring, normalizing, cleaning, and enriching.     
    · Validation rules refer to repetitive programming steps used to verify the consistency, quality, and security of the data you have. 

- This brings us to PUBLISHING, the fourth phase. Publishing involves delivering the output of the wrangled data for downstream project needs. What is published is the transformed and validated version of the input dataset along with the metadata about the dataset. 

- Lastly, it is important to note the criticality of DOCUMENTING the steps and considerations you have taken to convert the raw data to analysis-ready data. All phases of data wrangling are iterative in nature. In order to replicate the steps and to revisit your considerations for performing these steps, it is vital that you document all considerations and actions.



## TOOLS FOR DATA WRANGLING ##

> In this video, we will look at some of the popularly used data wrangling software and tools, such as: Excel Power Query / Spreadsheets, OpenRefine, Google DataPrep, Watson Studio Refinery, Trifacta Wrangler, Python and R. Let’s begin with the most basic software used for manual wrangling:

- SPREADSHEETS: Spreadsheets such as Microsoft Excel and Google Sheets, have a host of features and in-built formulae that can help you identify issues, clean, and transform data. Add-ins are available that allow you to import data from several different types of sources and clean and transform data as needed, such as Microsoft Power Query for Excel and Google Sheets Query function for Google Sheets. 

- OPENREFINE: OpenRefine is an open-source tool that allows you to import and export data in a wide variety of formats, such as TSV, CSV, XLS, XML, and JSON. Using OpenRefine, you can clean data, transform it from one format to another, and extend data with web services and external data. OpenRefine is easy to learn and easy to use. It offers menu-based operations, which means you don’t need to memorize commands or syntax. 

- GOOGLE DATAPREP: Google DataPrep is an intelligent cloud data service that allows you to visually explore, clean, and prepare both structured and unstructured data for analysis. It is a fully managed service, which means you don’t need to install or manage the software or the infrastructure. DataPrep is extremely easy to use. With every action that you take, you get suggestions on what your ideal next step should be. DataPrep can automatically detect schemas, data types, and anomalies. 

- WATSON STUDIO REFINERY: Watson Studio Refinery, available via IBM Watson Studio, allows you to discover, cleanse, and transform data with built-in operations. It transforms large amounts of raw data into consumable, quality information that’s ready for analytics. Data Refinery offers the flexibility of exploring data residing in a spectrum of data sources. It detects data types and classifications automatically and also enforces applicable data governance policies automatically. 

- TRIFACTA WRANGLER: Trifacta Wrangler is an interactive cloud-based service for cleaning and transforming data. It takes messy, real-world data and cleans and rearranges it into data tables, which can then be exported to Excel, Tableau, and R. It is known for its collaboration features, allowing multiple team members to work simultaneously. 

- PYTHON: Python has a huge library and set of packages that offer powerful data manipulation capabilities. Let’s look at a few of these:
    · Jupyter Notebook is an open-source web application widely used for data cleaning and transformation, statistical modeling, also data visualization. 
    · Numpy, or Numerical Python, is the most basic package that Python offers. It is fast, versatile, interoperable, and easy to use. It provides support for large, multi-dimensional arrays and matrices, and high-level mathematical functions to operate on these arrays. 
    · Pandas is designed for fast and easy data analysis operations. It allows complex operations such as merging, joining, and transforming huge chunks of data, performed using simple, single-line commands. Using Pandas, you can prevent common errors that result from misaligned data coming in from different sources. 
    
- R : Also offers a series of libraries and packages that are explicitly created for wrangling messy data—such as Dplyr, Data.table, and Jsonlite. Using these libraries, you can investigate, manipulate, and analyze data. 
    · Dplyr is a powerful library for data wrangling. It has a precise and straightforward syntax. 
    · Data.table helps to aggregate large data sets quickly. 
    · Jsonlite is a robust JSON parsing tool, great for interacting with web APIs. 
    

> Tools for data wrangling come with varying capabilities and dimensions. Your decision regarding the best tool for your needs will depend on factors that are specific to your use case, infrastructure, and teams—such as supported data size, data structures, cleaning and transformation capabilities, infrastructure needs, ease of use, and learnability.



## DATA CLEANING ##

- According to a Gartner report on data quality, poor quality data weakens an organization's competitive standing and undermines critical business objectives. Missing, inconsistent, or incorrect data can lead to false conclusions and therefore ineffective decisions. And in the business world, that can be costly. 

- Data sets picked up from disparate sources could have a number of issues, including missing values, inaccuracies, duplicates, incorrect or missing delimiters, inconsistent records, and insufficient parameters. In some cases, data can be corrected manually or automatically with the help of data wrangling tools and scripts, but if it cannot be repaired, it must be removed from the dataset. 

- Although the terms Data Cleaning and Data Wrangling are sometimes used interchangeably, it is important to keep in mind that data cleaning is only a subset of the entire Data Wrangling process. Data Cleaning forms a very significant and integral part of the Transformation phase in a data wrangling workflow. A typical data cleaning workflow includes: Inspection, Cleaning, and Verification. 

- The first step in the data cleaning workflow is to detect the different types of issues and errors that your dataset may have. You can use scripts and tools that allow you to define specific rules and constraints and validate your data against these rules and constraints. You can also use data profiling and data visualization tools for inspection. 
    · Data PROFILING helps you to inspect the source data to understand the structure, content, and interrelationships in your data. It uncovers anomalies and data quality issues. For example, blank or null values, duplicate data, or whether the value of a field falls within the expected range. Visualizing the data using statistical methods can help you to spot outliers. For example, plotting the average income in a demographic dataset can help you spot outliers. That brings us to the actual cleaning of the data. 

- The techniques you apply for cleaning your dataset will depend on your use case and the type of issues you encounter. 
Let’s look at some of the more common data ISSUES:

    · MISSING VALUES are very important to deal with as they can cause unexpected or biased results. You can choose to filter out the records with missing values or find a way to source that information in case it is intrinsic to your use case. For example, missing age data from a demographics study. A third option is a method known as imputation, which calculates the missing value based on statistical values. Your decision on the course of action you choose needs to be anchored in what’s best for your use case. 
    
    · DUPLICATE DATA are data points that are repeated in your dataset. These need to be removed. 
    
    · IRRELEVANT DATA is data that does not fit within the context of your use case can be considered irrelevant data. For example, if you are analyzing data about the general health of a segment of the population, their contact numbers may not be relevant for you. 
    
    · DATA TYPE CONVERSION, because is needed to ensure that values in a field are stored as the data type of that field. For example, numbers stored as numerical data type or date stored as a date data type. You may also need to clean your data in order to standardize it. For example, for strings, you may want all values to be in lower case. Similarly, date formats and units of measurement need to be standardized. 
    
    · SYNTAX ERRORS. For example, white spaces, or extra spaces at the beginning or end of a string is a syntax error that needs to be rectified. This can also include fixing typos or format. For example, the state name being entered as a full form such as New York versus an abbreviated form such as NY in some records. 
    
    · OUTLIERS, or values that are vastly different from other observations in the dataset. Outliers may, or may not, be incorrect. For example, when an age field in a voters database has the value 5, you know it is incorrect data and needs to be corrected. Now let’s consider a group of people where the annual income is in the range of one hundred thousand to two hundred thousand dollars—except for that one person who earns a million dollars a year. While this data point is not incorrect, it is an outlier, and needs to be looked at. Depending on your use case, you may need to decide if including this data will skew the results in a way that does not serve your use case. 
    
- This brings us to the next step in the data cleaning workflow: VERIFICATION. 
    · In this step, you inspect the results to establish effectiveness and accuracy achieved as a result of the data cleaning operation. You need to re-inspect the data to make sure the rules and constraints applicable on the data still hold after the corrections you made. 
    
- And in the end, it is important to note that all changes undertaken as part of the data cleaning operation need to be DOCUMENTED. 
    · Not just the changes, but also the reasons behind making those changes, and the quality of the currently stored data. 
    · Reporting how healthy the data is, is a very crucial step.
